import pandas as pd
from disciplines import generate_disciplines
from ai import enrich_discipline_metadata
from competencies import detect_competencies


# ============================================================
# 0. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–µ–π –¥–∏—Å—Ü–∏–ø–ª–∏–Ω
# ============================================================

def remove_duplicates(discs):
    """
    –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é.
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–µ—Ä–≤—É—é –≤—Å—Ç—Ä–µ—á–µ–Ω–Ω—É—é –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—É.
    """
    seen = set()
    unique = []
    for d in discs:
        name = d["name"].strip().lower()
        if name not in seen:
            seen.add(name)
            unique.append(d)
    return unique


# ============================================================
# 1. –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–µ–º–µ—Å—Ç—Ä–∞–º
# ============================================================

def balanced_distribution(obligatory, variative):
    """
    –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω –ø–æ —Å–µ–º–µ—Å—Ç—Ä–∞–º.
    –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ ‚Üí 1‚Äì6
    –í–∞—Ä–∏–∞—Ç–∏–≤–Ω—ã–µ ‚Üí 3‚Äì7
    """
    semester_plan = {s: [] for s in range(1, 8)}

    # –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ
    sems_obl = [1, 2, 3, 4, 5, 6]
    for i, disc in enumerate(obligatory):
        semester_plan[sems_obl[i % len(sems_obl)]].append(disc)

    # –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω—ã–µ
    sems_var = [3, 4, 5, 6, 7]
    for i, disc in enumerate(variative):
        semester_plan[sems_var[i % len(sems_var)]].append(disc)

    return semester_plan


# ============================================================
# 2. –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã –∫–æ–Ω—Ç—Ä–æ–ª—è
# ============================================================

def assign_assessment(name):
    name = name.lower()

    exam_keywords = [
        "–º–∞—Ç–µ–º–∞—Ç", "–º–µ—Ö–∞–Ω–∏–∫", "–ø—Ä–æ–≥—Ä–∞–º–º", "–∞–ª–≥–æ—Ä–∏—Ç–º",
        "–±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö", "sql", "nosql", "–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä",
        "—Å–µ—Ç", "–±–µ–∑–æ–ø–∞—Å", "–º–∞—à–∏–Ω", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"
    ]

    diff_keywords = [
        "–≥—Ä–∞—Ñ–∏–∫", "–≤–µ–∫—Ç–æ—Ä", "—Ä–∞—Å—Ç—Ä–æ–≤", "–º—É–ª—å—Ç–∏–º–µ–¥",
        "–≤–µ–±", "api", "cms", "ux", "ui"
    ]

    soft_keywords = [
        "–∫—É–ª—å—Ç—É—Ä", "–∏—Å—Ç–æ—Ä", "—Ñ–∏–ª–æ—Å–æ—Ñ", "–ø—Å–∏—Ö–æ–ª–æ–≥",
        "–∫–æ–º–º—É–Ω–∏–∫", "soft", "—Å–∞–º–æ–º–µ–Ω–µ–¥–∂", "—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–µ–º"
    ]

    if any(k in name for k in exam_keywords):
        return "—ç–∫–∑–∞–º–µ–Ω"
    if any(k in name for k in diff_keywords):
        return "–¥–∏—Ñ. –∑–∞—á—ë—Ç"
    if any(k in name for k in soft_keywords):
        return "–∑–∞—á—ë—Ç"

    return "–∑–∞—á—ë—Ç"


# ============================================================
# 3. –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ —É—á–µ–±–Ω–æ–≥–æ –ø–ª–∞–Ω–∞
# ============================================================

def generate_plan_pipeline(df_fgos, tf_struct, match_json, fgos_text):
    """
    –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω:
    - –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª—è
    - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∏—Å—Ü–∏–ø–ª–∏–Ω
    - enrich –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–µ–º–µ—Å—Ç—Ä–∞–º
    - –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ —Ñ–æ—Ä–º –∫–æ–Ω—Ç—Ä–æ–ª—è
    - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π
    - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–∫—Ç–∏–∫ –∏ –ì–ò–ê
    """

    # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ—Ñ–∏–ª—å
    from fgos import detect_profile_from_fgos
    profile = detect_profile_from_fgos(fgos_text)[0]

    # 2. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã
    discs = generate_disciplines(profile)

    # 3. –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏
    discs = remove_duplicates(discs)

    # 4. enrich –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (TF + –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ)
    enriched = {d["name"]: enrich_discipline_metadata(d, df_fgos, tf_struct) for d in discs}

    # 5. –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –±–ª–æ–∫–∞–º (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–∫—Ç—ã, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –∏–º–µ–Ω–∞)
    obligatory = [d for d in discs if d["block_hint"] == "–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è"]
    variative = [d for d in discs if d["block_hint"] == "–≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–∞—è"]

    # 6. –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
    semester_map = balanced_distribution(obligatory, variative)

    # 7. –°–±–æ—Ä–∫–∞ —Å—Ç—Ä–æ–∫
    rows = []

    for sem, disc_list in semester_map.items():
        for disc in disc_list:
            name = disc["name"]
            meta = enriched.get(name, {})

            rows.append({
                "–ë–ª–æ–∫": f"–ë–ª–æ–∫ 1. {'–û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è' if disc['block_hint']=='–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è' else '–í–∞—Ä–∏–∞—Ç–∏–≤–Ω–∞—è'} —á–∞—Å—Ç—å",
                "–°–µ–º–µ—Å—Ç—Ä": sem,
                "–î–∏—Å—Ü–∏–ø–ª–∏–Ω–∞": name,
                "–ß–∞—Å—ã": 144 if disc["block_hint"] == "–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è" else 108,
                "–§–æ—Ä–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è": assign_assessment(name),

                # üî• –ù–æ–≤–∞—è –ª–æ–≥–∏–∫–∞ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π
                "–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –§–ì–û–°": detect_competencies(profile, name),

                # TF –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ ‚Äî –∏–∑ enrich
                "–¢—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏": ", ".join(meta.get("TF", [])),
                "–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ": meta.get("reason", "")
            })

    # 8. –ü—Ä–∞–∫—Ç–∏–∫–∞ + –ì–ò–ê
    rows.extend([
        {
            "–ë–ª–æ–∫": "–ë–ª–æ–∫ 2. –ü—Ä–∞–∫—Ç–∏–∫–∞",
            "–°–µ–º–µ—Å—Ç—Ä": 7,
            "–î–∏—Å—Ü–∏–ø–ª–∏–Ω–∞": "–£—á–µ–±–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞",
            "–ß–∞—Å—ã": 108,
            "–§–æ—Ä–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è": "–∑–∞—á—ë—Ç",
            "–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –§–ì–û–°": "",
            "–¢—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏": "",
            "–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ": "–ü—Ä–∞–∫—Ç–∏–∫–∞ –ø–æ –§–ì–û–°"
        },
        {
            "–ë–ª–æ–∫": "–ë–ª–æ–∫ 2. –ü—Ä–∞–∫—Ç–∏–∫–∞",
            "–°–µ–º–µ—Å—Ç—Ä": 8,
            "–î–∏—Å—Ü–∏–ø–ª–∏–Ω–∞": "–ü—Ä–µ–¥–¥–∏–ø–ª–æ–º–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞",
            "–ß–∞—Å—ã": 108,
            "–§–æ—Ä–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è": "–∑–∞—á—ë—Ç",
            "–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –§–ì–û–°": "",
            "–¢—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏": "",
            "–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ": "–ü—Ä–µ–¥–¥–∏–ø–ª–æ–º–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –ø–æ –§–ì–û–°"
        },
        {
            "–ë–ª–æ–∫": "–ë–ª–æ–∫ 3. –ì–ò–ê",
            "–°–µ–º–µ—Å—Ç—Ä": 8,
            "–î–∏—Å—Ü–∏–ø–ª–∏–Ω–∞": "–í–ö–†",
            "–ß–∞—Å—ã": 216,
            "–§–æ—Ä–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è": "–∑–∞—â–∏—Ç–∞",
            "–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –§–ì–û–°": "",
            "–¢—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏": "",
            "–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ": "–ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–∞—è –∏—Ç–æ–≥–æ–≤–∞—è –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏—è"
        }
    ])

    return pd.DataFrame(rows)
