import streamlit as st
import fitz  # pymupdf
import re
import pandas as pd
import io
import requests
import json
import pytesseract
from pdf2image import convert_from_bytes
from PIL import Image


import streamlit as st

YANDEX_API_KEY = st.secrets["YANDEX_API_KEY"]

FOLDER_ID = "b1gduq5bjgu0km5qubgu"
YANDEX_API_URL = "https://llm.api.cloud.yandex.net/foundationModels/v1/completion"
YANDEX_MODEL_LITE = "yandexgpt-lite"

# –ü—É—Ç—å –∫ Tesseract (Windows)
pytesseract.pytesseract.tesseract_cmd = r"C:\\Program Files\\Tesseract-OCR\\tesseract.exe"


def call_yandex_lite(messages, temperature=0.3, max_tokens=1500):
    headers = {
        "Authorization": f"Api-Key {YANDEX_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "modelUri": f"gpt://{FOLDER_ID}/{YANDEX_MODEL_LITE}",
        "completionOptions": {
            "stream": False,
            "temperature": temperature,
            "maxTokens": max_tokens
        },
        "messages": messages
    }
    try:
        response = requests.post(YANDEX_API_URL, headers=headers, json=data, timeout=90)
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ YandexGPT: {e}"

    try:
        result = response.json()
    except Exception:
        return "–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç YandexGPT –∫–∞–∫ JSON."

    if "error" in result:
        return f"–û—à–∏–±–∫–∞ —Å–µ—Ä–≤–∏—Å–∞ YandexGPT: {result.get('error')}"

    try:
        return result["result"]["alternatives"][0]["message"]["text"]
    except Exception:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–æ—Ä–µ –æ—Ç–≤–µ—Ç–∞ YandexGPT: {result}"


def extract_text_from_pdf_file(uploaded_file):
    """
    1) –ü—ã—Ç–∞–µ—Ç—Å—è –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ MuPDF.
    2) –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –º–∞–ª–æ (—Å–∫–∞–Ω) ‚Äî –≤–∫–ª—é—á–∞–µ—Ç OCR (Tesseract).
    """
    uploaded_file.seek(0)
    pdf_bytes = uploaded_file.read()

    text = ""
    try:
        with fitz.open(stream=pdf_bytes, filetype="pdf") as doc:
            for page in doc:
                text += page.get_text()
    except Exception:
        text = ""

    if len(text.strip()) > 50:
        return text

    # OCR
    ocr_text = ""
    try:
        images = convert_from_bytes(pdf_bytes)
        for img in images:
            ocr_text += pytesseract.image_to_string(img, lang="rus+eng") + "\n"
    except Exception as e:
        return f"OCR error: {e}"

    return ocr_text


def extract_competencies_full(text):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –£–ö, –û–ü–ö, –ü–ö –∏–∑ —Ç–µ–∫—Å—Ç–∞ –§–ì–û–°.
    """
    pattern = (
        r"(–£–ö-\d+|–û–ü–ö-\d+|–ü–ö-\d+)\.?\s*(.*?)\s*"
        r"(?=(–£–ö-\d+|–û–ü–ö-\d+|–ü–ö-\d+|IV\. –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —É—Å–ª–æ–≤–∏—è–º —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º—ã –±–∞–∫–∞–ª–∞–≤—Ä–∏–∞—Ç–∞|$))"
    )
    matches = re.findall(pattern, text, re.DOTALL)

    competencies = []
    for code, desc, _ in matches:
        desc = " ".join(desc.split())
        competencies.append({"code": code, "description": desc})
    return competencies


def dataframe_to_excel_bytes(df):
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False)
    return output.getvalue()




def extract_tf_codes_smart(full_text):
    """
    –ò—â–µ—Ç –∫–æ–¥—ã –¢–§ –≤ –ª—é–±–æ–º –≤–∏–¥–µ (—Å–∫–∞–Ω—ã, —Ç–∞–±–ª–∏—Ü—ã, —Ä–∞–∑—Ä—ã–≤—ã).
    –ü—Ä–∏–º–µ—Ä—ã:
    A/01.3, A 01.3, A-01-3, A01.3 –∏ —Ç.–ø.
    """
    pattern = r"\b([A-D])\s*[/\-‚Äì‚Äî]?\s*(\d{2})\s*[\.\-‚Äì‚Äî,¬∑]?\s*(\d)\b"
    matches = re.findall(pattern, full_text)

    codes = []
    for letter, num1, num2 in matches:
        code = f"{letter}/{num1}.{num2}"
        if code not in codes:
            codes.append(code)

    return codes


def get_context_for_tf(full_text, tf_code, window=25):
    """
    –ù–∞—Ö–æ–¥–∏—Ç —Å—Ç—Ä–æ–∫—É —Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º –∫–æ–¥–æ–º –¢–§ (A/01.3),
    —É—á–∏—Ç—ã–≤–∞—è, —á—Ç–æ –≤ —Ç–µ–∫—Å—Ç–µ –æ–Ω –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞–ø–∏—Å–∞–Ω –ø–æ-—Ä–∞–∑–Ω–æ–º—É.
    """
    lines = full_text.split("\n")

    letter, nums = tf_code.split("/")
    num1, num2 = nums.split(".")

    flex_pattern = rf"{letter}\s*[/\-‚Äì‚Äî]?\s*{num1}\s*[\.\-‚Äì‚Äî,¬∑]?\s*{num2}"

    indices = [i for i, line in enumerate(lines) if re.search(flex_pattern, line)]

    if not indices:
        return ""

    i = indices[0]
    start = max(0, i - window)
    end = min(len(lines), i + window)
    return "\n".join(lines[start:end])


def analyze_single_tf_with_ai(tf_code, context_text):
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞ –ø–æ –æ–¥–Ω–æ–π –¢–§ –≤ –ò–ò –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É.
    """
    prompt = f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –†–§.

–ù–∞ –≤—Ö–æ–¥–µ ‚Äî —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞,
–æ—Ç–Ω–æ—Å—è—â–∏–π—Å—è –∫ —Ç—Ä—É–¥–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ —Å –∫–æ–¥–æ–º {tf_code}.

–ò–∑–≤–ª–µ–∫–∏ —Å—Ç—Ä–æ–≥–æ –ø–æ —ç—Ç–æ–º—É –∫–æ–¥—É:
- name
- actions
- knowledge
- skills
- other

–í–µ—Ä–Ω–∏ —Å—Ç—Ä–æ–≥–æ JSON.
"""

    raw = call_yandex_lite(
        [{"role": "user", "text": prompt + context_text[:6000]}],
        max_tokens=1200,
        temperature=0.2
    )

    try:
        start = raw.index("{")
        end = raw.rindex("}") + 1
        data = json.loads(raw[start:end])
    except:
        return {
            "code": tf_code,
            "name": "",
            "actions": [],
            "knowledge": [],
            "skills": [],
            "other": []
        }

    return {
        "code": tf_code,
        "name": data.get("name", ""),
        "actions": data.get("actions", []),
        "knowledge": data.get("knowledge", []),
        "skills": data.get("skills", []),
        "other": data.get("other", [])
    }


def analyze_prof_standard(full_text):
    tf_codes = extract_tf_codes_smart(full_text)
    if not tf_codes:
        return None, "–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞ –¢–§."

    tf_list = []
    for code in tf_codes:
        context = get_context_for_tf(full_text, code)
        tf_list.append(analyze_single_tf_with_ai(code, context))

    return {"TF": tf_list}, None
# =========================
# –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –§–ì–û–° ‚Üî –ø—Ä–æ—Ñ—Å—Ç–∞–Ω–¥–∞—Ä—Ç (Lite)
# =========================

def match_fgos_and_prof(df_fgos, tf_struct):
    """
    –£—Å—Ç–æ–π—á–∏–≤–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –§–ì–û–° ‚Üî –ø—Ä–æ—Ñ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞ –Ω–∞ YandexGPT Lite.
    """

    fgos_short = df_fgos.copy()
    fgos_short["description"] = fgos_short["description"].apply(lambda x: str(x)[:300])

    tf_short = []
    for tf in tf_struct.get("TF", []):
        tf_short.append({
            "code": tf.get("code", ""),
            "name": (tf.get("name") or "")[:200],
            "actions": (tf.get("actions") or [])[:5],
            "knowledge": (tf.get("knowledge") or [])[:5],
            "skills": (tf.get("skills") or [])[:5],
        })

    prompt_match = f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –†–§.

–¢–µ–±–µ –¥–∞–Ω—ã:
- –ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –§–ì–û–° (–∫–æ–¥—ã –∏ –æ–ø–∏—Å–∞–Ω–∏—è).
- –¢—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞ (–∫–æ–¥, –Ω–∞–∑–≤–∞–Ω–∏–µ, –¥–µ–π—Å—Ç–≤–∏—è, –∑–Ω–∞–Ω–∏—è, —É–º–µ–Ω–∏—è).

–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –∏ —Ç—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏.

–í–µ—Ä–Ω–∏ —Å—Ç—Ä–æ–≥–æ JSON:

{{
  "matches": [
    {{
      "competency": "–£–ö-1",
      "related_TF": ["A/01.3"],
      "comment": "–ö—Ä–∞—Ç–∫–æ–µ –ø–æ—è—Å–Ω–µ–Ω–∏–µ"
    }}
  ],
  "gaps": [
    {{
      "TF": "B/03.4",
      "reason": "–ü–æ—á–µ–º—É –Ω–µ –ø–æ–∫—Ä—ã—Ç–∞"
    }}
  ],
  "recommendations": ["–ö—Ä–∞—Ç–∫–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è"]
}}

–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –§–ì–û–°:
{fgos_short.to_json(orient="records", force_ascii=False)}

–¢—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:
{json.dumps(tf_short, ensure_ascii=False)}
"""

    raw = call_yandex_lite(
        [{"role": "user", "text": prompt_match}],
        temperature=0.25,
        max_tokens=1800
    )

    try:
        start = raw.index("{")
        end = raw.rindex("}") + 1
        data = json.loads(raw[start:end])
        return {
            "matches": data.get("matches", []),
            "gaps": data.get("gaps", []),
            "recommendations": data.get("recommendations", [])
        }, None
    except Exception:
        return {
            "matches": [],
            "gaps": [],
            "recommendations": ["–ò–ò –≤–µ—Ä–Ω—É–ª –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π JSON –ø—Ä–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏."]
        }, raw


def cluster_competencies_and_tf(df_fgos, tf_struct, profile_choice="–ê–≤—Ç–æ"):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π –∏ —Ç—Ä—É–¥–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –ø–æ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –≥—Ä—É–ø–ø–∞–º.
    –†–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –ª—é–±–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è.
    """

    prompt = f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –∏ –ø—Ä–æ—Ñ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –†–§.

–¢–µ–±–µ –¥–∞–Ω—ã:
- –ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –§–ì–û–° (–∫–æ–¥ + –æ–ø–∏—Å–∞–Ω–∏–µ),
- –¢—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞ (–∫–æ–¥ + –Ω–∞–∑–≤–∞–Ω–∏–µ + —ç–ª–µ–º–µ–Ω—Ç—ã —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è),
- –ü—Ä–æ—Ñ–∏–ª—å –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏: "{profile_choice}".

–ó–∞–¥–∞—á–∞:
1. –†–∞–∑–±–µ–π –í–°–ï –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –§–ì–û–° –∏ —Ç—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–∞ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä—É–ø–ø—ã (–∫–ª–∞—Å—Ç–µ—Ä—ã).
2. –î–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã –æ–ø–∏—à–∏ –µ—ë —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ñ–æ–∫—É—Å.
3. –ü—Ä–∏–≤—è–∂–∏ –∫ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø–µ:
   - —Å–ø–∏—Å–æ–∫ –∫–æ–¥–æ–≤ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π,
   - —Å–ø–∏—Å–æ–∫ –∫–æ–¥–æ–≤ —Ç—Ä—É–¥–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π.

–í–µ—Ä–Ω–∏ —Å—Ç—Ä–æ–≥–æ JSON:

{{
  "clusters": [
    {{
      "name": "–ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã",
      "description": "–û–ø–∏—Å–∞–Ω–∏–µ",
      "competencies": ["–£–ö-1"],
      "TF": ["A/01.3"]
    }}
  ]
}}

–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –§–ì–û–°:
{df_fgos.to_json(orient="records", force_ascii=False)}

–¢—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:
{json.dumps(tf_struct.get("TF", []), ensure_ascii=False)}
"""

    raw = call_yandex_lite(
        [{"role": "user", "text": prompt}],
        max_tokens=2500,
        temperature=0.25
    )

    try:
        start = raw.index("{")
        end = raw.rindex("}") + 1
        data = json.loads(raw[start:end])
        clusters = data.get("clusters", [])
        return clusters if isinstance(clusters, list) else []
    except:
        return []


def generate_disciplines_for_cluster(
    cluster, df_fgos, tf_struct, profile_choice="–ê–≤—Ç–æ",
    min_count=6, max_count=12
):
    cluster_name = cluster.get("name", "–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º–æ–¥—É–ª—å")
    cluster_desc = cluster.get("description", "")
    comp_codes = cluster.get("competencies", [])
    tf_codes = cluster.get("TF", [])

    prompt = f"""
–¢—ã ‚Äî –º–µ—Ç–æ–¥–∏—Å—Ç –≤—É–∑–∞ –†–§.

–ü—Ä–æ—Ñ–∏–ª—å: "{profile_choice}".
–ö–ª–∞—Å—Ç–µ—Ä: "{cluster_name}".
–û–ø–∏—Å–∞–Ω–∏–µ: "{cluster_desc}".

–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏: {comp_codes}
–¢—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏: {tf_codes}

–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π {min_count}‚Äì{max_count} –¥–∏—Å—Ü–∏–ø–ª–∏–Ω.

–í–∞–∂–Ω–æ:
- –ù–ï —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–π –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã –ø–æ –±–ª–æ–∫–∞–º.
- –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π —Å–ª–æ–≤–∞ "–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è" –∏–ª–∏ "–≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–∞—è".
- –ü—Ä–æ—Å—Ç–æ –¥–∞–π —Å–ø–∏—Å–æ–∫ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω.

–í–µ—Ä–Ω–∏ —Å—Ç—Ä–æ–≥–æ JSON:
{{
  "disciplines": [
    {{
      "name": "–ù–∞–∑–≤–∞–Ω–∏–µ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã",
      "competencies": ["–£–ö-1"],
      "TF": ["A/01.3"]
    }}
  ]
}}
"""

    raw = call_yandex_lite(
        [{"role": "user", "text": prompt}],
        max_tokens=2000,
        temperature=0.3
    )

    try:
        start = raw.index("{")
        end = raw.rindex("}") + 1
        discs = json.loads(raw[start:end]).get("disciplines", [])
    except:
        return []

    # --- —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –±–ª–æ–∫–æ–≤ Python ---
    obligatory = []
    variative = []

    for d in discs:
        name = d.get("name", "").lower()

        if is_fundamental(name):
            d["block_hint"] = "–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è"
            obligatory.append(d)
        else:
            d["block_hint"] = "–≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–∞—è"
            variative.append(d)

    return obligatory + variative

def rebalance_blocks(all_discs):
    obligatory = [d for d in all_discs if d["block_hint"] == "–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è"]
    variative = [d for d in all_discs if d["block_hint"] == "–≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–∞—è"]

    # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö ‚â§ 20
    if len(obligatory) > 20:
        extra = obligatory[20:]
        for d in extra:
            d["block_hint"] = "–≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–∞—è"
        obligatory = obligatory[:20]
        variative.extend(extra)

    # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω—ã—Ö ‚â• 20
    if len(variative) < 20:
        need = 20 - len(variative)
        candidates = [d for d in obligatory if not is_fundamental(d["name"])]
        for d in candidates[:need]:
            d["block_hint"] = "–≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–∞—è"
            variative.append(d)
            obligatory.remove(d)

    return obligatory + variative
def is_fundamental(name: str):
    name = name.lower()

    # 1. –§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
    fundamental_keywords = [
        "–º–∞—Ç–µ–º–∞—Ç", "–∞–Ω–∞–ª–∏–∑", "—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫", "–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç",
        "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–π", "–æ–ø—Ç–∏–º–∏–∑–∞—Ü", "–º–µ—Ö–∞–Ω–∏–∫",
        "–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω", "—Ä–∞—Å—á—ë—Ç", "—Ä–∞—Å—á–µ—Ç",
        "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω", "python", "–∞–ª–≥–æ—Ä–∏—Ç–º", "—Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö",
        "–º–∏–∫—Ä–æ–∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä", "–∫–æ–¥–∏—Ä–æ–≤–∞–Ω",
        "–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä", "–æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω", "—Å–∏—Å—Ç–µ–º–Ω", "—ç–ª–µ–∫—Ç—Ä–æ—Ç–µ—Ö–Ω", "—ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫",
        "—Å–µ—Ç", "network", "–±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö", "sql", "nosql", "—Å—É–±–¥",
        "–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫", "—Ç–µ–æ—Ä–∏—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è", "—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏",
        "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "it", "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω —Å–∏—Å—Ç–µ–º",
    ]

    # 2. –ü—Ä–∏–∫–ª–∞–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Ç–æ–ª—å–∫–æ soft‚Äëskills –∏ –≥—É–º–∞–Ω–∏—Ç–∞—Ä–∫–∞)
    variative_keywords = [
        "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥", "–ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å", "–±–∏–∑–Ω–µ—Å",
        "—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–æ–º", "—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–∞–º–∏",
        "—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–ª–æ–º", "–º–µ–¥–∏–∞–ø–ª–∞–Ω",
        "–∫–æ–≥–Ω–∏—Ç–∏–≤", "–∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–ª–æ–≥",
        "—Å—Ç—Ä–µ—Å—Å", "—á—Ç–µ–Ω–∏–µ", "–ø–∏—Å—å–º–æ",
        "—Ä–∏—Ç–æ—Ä–∏–∫–∞", "–æ—Ä–∞—Ç–æ—Ä—Å–∫–æ–µ", "–ª–∏–¥–µ—Ä—Å—Ç–≤–æ",
        "–≥—Ä—É–ø–ø–æ–≤–∞—è –¥–∏–Ω–∞–º–∏–∫–∞", "–∫—É–ª—å—Ç—É—Ä", "–ø—Å–∏—Ö–æ–ª–æ–≥", "—Å–æ—Ü–∏–æ–ª–æ–≥",
        "—Ñ–∏–ª–æ—Å–æ—Ñ", "–∏—Å—Ç–æ—Ä–∏—è", "—ç–∫–æ–Ω–æ–º–∏–∫"
    ]

    # 3. –ï—Å–ª–∏ –µ—Å—Ç—å —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ‚Üí –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–ê–Ø
    if any(k in name for k in fundamental_keywords):
        return True

    # 4. –ï—Å–ª–∏ –µ—Å—Ç—å soft‚Äëskills ‚Üí –í–ê–†–ò–ê–¢–ò–í–ù–ê–Ø
    if any(k in name for k in variative_keywords):
        return False

    # 5. –í—Å—ë –æ—Å—Ç–∞–ª—å–Ω–æ–µ ‚Äî –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ–µ
    return False


def generate_universal_disciplines(df_fgos, tf_struct, match_json, profile_choice="–ê–≤—Ç–æ"):
    """
    –≠—Ç–∞–ø 1: –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è ‚Üí –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã ‚Üí —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–µ–π ‚Üí –¥–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è ‚Üí —Ä–µ–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –±–ª–æ–∫–æ–≤.
    """

    clusters = cluster_competencies_and_tf(df_fgos, tf_struct, profile_choice)
    if not clusters:
        return []

    # 1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∏—Å—Ü–∏–ø–ª–∏–Ω –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
    all_discs = []
    for cl in clusters:
        discs = generate_disciplines_for_cluster(cl, df_fgos, tf_struct, profile_choice)
        all_discs.extend(discs)

    # 2. –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏
    seen = set()
    unique = []
    for d in all_discs:
        name = (d.get("name") or "").strip()
        if name and name not in seen:
            seen.add(name)
            unique.append(d)

    # 3. –î–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è, –µ—Å–ª–∏ –º–∞–ª–æ
    if len(unique) < 40:
        prompt_more = f"""
–¢—ã —Ä–∞–Ω–µ–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª {len(unique)} –¥–∏—Å—Ü–∏–ø–ª–∏–Ω.
–ù—É–∂–Ω–æ –¥–æ–≤–µ—Å—Ç–∏ –¥–æ 45‚Äì55.

–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã.
JSON:
{{
  "disciplines": [
    {{
      "name": "–ù–∞–∑–≤–∞–Ω–∏–µ",
      "competencies": ["–£–ö-1"],
      "TF": ["A/01.3"],
      "block_hint": "–≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–∞—è"
    }}
  ]
}}
"""
        raw = call_yandex_lite(
            [{"role": "user", "text": prompt_more}],
            max_tokens=2000,
            temperature=0.35
        )
        try:
            start = raw.index("{")
            end = raw.rindex("}") + 1
            extra = json.loads(raw[start:end]).get("disciplines", [])
        except:
            extra = []

        for d in extra:
            name = (d.get("name") or "").strip()
            if name and name not in seen:
                seen.add(name)
                unique.append(d)

    
    unique = rebalance_blocks(unique)

    return unique


def generate_disciplines_from_fgos_and_prof(df_fgos, tf_struct, match_json, profile_choice):
    discs = generate_universal_disciplines(df_fgos, tf_struct, match_json, profile_choice)
    st.session_state.debug_disciplines = discs
    return discs


def assign_hours_and_assessment(disciplines):
    """
    Lite –Ω–∞–∑–Ω–∞—á–∞–µ—Ç —á–∞—Å—ã –∏ —Ñ–æ—Ä–º—É –∫–æ–Ω—Ç—Ä–æ–ª—è.
    """
    prompt = f"""
–¢—ã ‚Äî –º–µ—Ç–æ–¥–∏—Å—Ç –≤—É–∑–∞ –†–§.

–ù–∞–∑–Ω–∞—á—å –∫–∞–∂–¥–æ–π –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–µ:
- –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å–æ–≤ (72‚Äì216),
- —Ñ–æ—Ä–º—É –∫–æ–Ω—Ç—Ä–æ–ª—è: —ç–∫–∑–∞–º–µ–Ω / –∑–∞—á—ë—Ç / –¥–∏—Ñ. –∑–∞—á—ë—Ç.

–í–µ—Ä–Ω–∏ —Å—Ç—Ä–æ–≥–æ JSON:
{{
  "disciplines": [
    {{
      "name": "–ù–∞–∑–≤–∞–Ω–∏–µ",
      "hours": 144,
      "assessment": "—ç–∫–∑–∞–º–µ–Ω"
    }}
  ]
}}

–î–∞–Ω–Ω—ã–µ:
{json.dumps(disciplines, ensure_ascii=False, indent=2)}
"""

    raw = call_yandex_lite(
        [{"role": "user", "text": prompt}],
        temperature=0.2,
        max_tokens=2000
    )

    try:
        start = raw.index("{")
        end = raw.rindex("}") + 1
        return json.loads(raw[start:end]).get("disciplines", [])
    except:
        return disciplines

def assign_blocks(disciplines):
    blocks = []
    for d in disciplines:
        if d.get("block_hint") == "–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è":
            blocks.append({"name": d["name"], "block": "–ë–ª–æ–∫ 1. –û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å"})
        else:
            blocks.append({"name": d["name"], "block": "–ë–ª–æ–∫ 1. –í–∞—Ä–∏–∞—Ç–∏–≤–Ω–∞—è —á–∞—Å—Ç—å"})
    return blocks




def enrich_discipline_metadata(discipline, df_fgos, tf_struct):
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è: –ø–µ—Ä–µ–¥–∞—ë–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –§–ì–û–° –∏ –¢–§,
    —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –≤—ã–±–∏—Ä–∞–ª–∞ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ.
    """

    prompt = f"""
–¢—ã ‚Äî –º–µ—Ç–æ–¥–∏—Å—Ç –≤—É–∑–∞ –†–§.

–¢–µ–±–µ –¥–∞–Ω–∞ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞: "{discipline['name']}".

–í–æ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π –§–ì–û–°:
{df_fgos.to_json(orient="records", force_ascii=False)}

–í–æ—Ç —Å–ø–∏—Å–æ–∫ —Ç—Ä—É–¥–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –ø—Ä–æ—Ñ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞:
{json.dumps(tf_struct.get("TF", []), ensure_ascii=False)}

–û–ø—Ä–µ–¥–µ–ª–∏:
- –∫–∞–∫–∏–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —ç—Ç–∞ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞ (2‚Äì4 –∫–æ–¥–∞),
- –∫–∞–∫–∏–µ —Ç—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ–Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç (0‚Äì3 –∫–æ–¥–∞),
- –∫–æ—Ä–æ—Ç–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ (–¥–æ 150 —Å–∏–º–≤–æ–ª–æ–≤).

–í–µ—Ä–Ω–∏ —Å—Ç—Ä–æ–≥–æ JSON:
{{
  "competencies": ["–£–ö-1", "–û–ü–ö-2"],
  "TF": ["A/01.3"],
  "reason": "–ö–æ—Ä–æ—Ç–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ"
}}
"""

    raw = call_yandex_lite(
        [{"role": "user", "text": prompt}],
        temperature=0.2,
        max_tokens=700
    )

    try:
        start = raw.index("{")
        end = raw.rindex("}") + 1
        return json.loads(raw[start:end])
    except:
        return {
            "competencies": [],
            "TF": [],
            "reason": ""
        }



def prepare_block_structure(disciplines_with_hours):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É:
    {
        "–ë–ª–æ–∫ 1. –û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å": [—Å–ø–∏—Å–æ–∫ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω],
        "–ë–ª–æ–∫ 1. –í–∞—Ä–∏–∞—Ç–∏–≤–Ω–∞—è —á–∞—Å—Ç—å": [—Å–ø–∏—Å–æ–∫ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω]
    }
    """
    block_assignments = assign_blocks(disciplines_with_hours)

    block_map = {
        "–ë–ª–æ–∫ 1. –û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å": [],
        "–ë–ª–æ–∫ 1. –í–∞—Ä–∏–∞—Ç–∏–≤–Ω–∞—è —á–∞—Å—Ç—å": []
    }

    for item in block_assignments:
        name = item.get("name")
        block = item.get("block", "–ë–ª–æ–∫ 1. –û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å")
        if name:
            block_map.setdefault(block, []).append(name)

    return block_map


def distribute_evenly(items, semester_ranges):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è.
    items ‚Äî —Å–ø–∏—Å–æ–∫ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω
    semester_ranges ‚Äî —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ —Å–µ–º–µ—Å—Ç—Ä–æ–≤, –Ω–∞–ø—Ä–∏–º–µ—Ä:
        [[1,2,3], [4,5], [6], [7], [8]]
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å: {–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞: —Å–µ–º–µ—Å—Ç—Ä}
    """
    result = {}
    idx = 0

    for group in semester_ranges:
        count = len(group)
        # —Å–∫–æ–ª—å–∫–æ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω –ø–æ–ª–æ–∂–∏—Ç—å –≤ —ç—Ç—É –≥—Ä—É–ø–ø—É
        portion = max(1, len(items) // len(semester_ranges))

        # –µ—Å–ª–∏ –º–∞–ª–æ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω ‚Äî —É–º–µ–Ω—å—à–∞–µ–º
        if portion > len(items) - idx:
            portion = len(items) - idx

        selected = items[idx: idx + portion]
        idx += portion

        
        for i, disc in enumerate(selected):
            sem = group[i % count]
            result[disc] = sem

        if idx >= len(items):
            break

    
    if idx < len(items):
        tail = items[idx:]
        last_group = semester_ranges[-1]
        for i, disc in enumerate(tail):
            result[disc] = last_group[i % len(last_group)]

    return result


def distribute_by_semesters(block_map):
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω –ø–æ —Å–µ–º–µ—Å—Ç—Ä–∞–º.
    block_map:
    {
        "–ë–ª–æ–∫ 1. –û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å": [...],
        "–ë–ª–æ–∫ 1. –í–∞—Ä–∏–∞—Ç–∏–≤–Ω–∞—è —á–∞—Å—Ç—å": [...]
    }
    """

    # 1) –û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å ‚Äî —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ –ø–æ 1‚Äì6 —Å–µ–º–µ—Å—Ç—Ä–∞–º
    obligatory = block_map.get("–ë–ª–æ–∫ 1. –û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å", [])
    obligatory_semesters = distribute_evenly(
        obligatory,
        [
            [1, 2],   # —Ä–∞–Ω–Ω–∏–µ —Å–µ–º–µ—Å—Ç—Ä—ã
            [3, 4],   # —Å–µ—Ä–µ–¥–∏–Ω–∞
            [5, 6]    # –ø–æ–∑–¥–Ω–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ
        ]
    )

    # 2) –í–∞—Ä–∏–∞—Ç–∏–≤–Ω–∞—è —á–∞—Å—Ç—å ‚Äî —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ –ø–æ 3‚Äì7 —Å–µ–º–µ—Å—Ç—Ä–∞–º
    variative = block_map.get("–ë–ª–æ–∫ 1. –í–∞—Ä–∏–∞—Ç–∏–≤–Ω–∞—è —á–∞—Å—Ç—å", [])
    variative_semesters = distribute_evenly(
        variative,
        [
            [3, 4],   # –Ω–∞—á–∞–ª–æ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω—ã—Ö
            [5, 6],   # —Å–µ—Ä–µ–¥–∏–Ω–∞
            [7]       # –ø–æ–∑–¥–Ω–∏–µ
        ]
    )

    # –æ–±—ä–µ–¥–∏–Ω—è–µ–º
    semester_map = {}
    semester_map.update(obligatory_semesters)
    semester_map.update(variative_semesters)

    return semester_map



def assign_semesters_and_blocks(disciplines_with_hours, df_fgos, tf_struct, structure_mode="old"):
    """
    –ü–æ–ª–Ω–æ—Å—Ç—å—é –Ω–æ–≤—ã–π –≠—Ç–∞–ø 3:
    - –±–ª–æ–∫–∏ (–ò–ò)
    - –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏/–¢–§/–æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ (–ò–ò)
    - —Å–µ–º–µ—Å—Ç—Ä—ã (Python, –∏–¥–µ–∞–ª—å–Ω–æ)
    """

    # 3.1 ‚Äî –±–ª–æ–∫–∏
    block_map = prepare_block_structure(disciplines_with_hours)

    # 3.2 ‚Äî —Å–µ–º–µ—Å—Ç—Ä—ã (Python)
    semester_map = distribute_by_semesters(block_map)

    # 3.3 ‚Äî –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–ò–ò)
    enriched = {}
    for d in disciplines_with_hours:
        enriched[d["name"]] = enrich_discipline_metadata(d, df_fgos, tf_struct)

    # 3.4 ‚Äî —Å–±–æ—Ä–∫–∞ —Å—Ç—Ä–æ–∫
    rows = []
    for d in disciplines_with_hours:
        name = d["name"]

        # –±–ª–æ–∫
        block = None
        for b, names in block_map.items():
            if name in names:
                block = b
                break
        if block is None:
            block = "–ë–ª–æ–∫ 1. –û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å"

        # —Å–µ–º–µ—Å—Ç—Ä
        semester = semester_map.get(name, 1)

        meta = enriched.get(name, {})

        rows.append({
            "üõ† –í —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ": block,
            "–°–µ–º–µ—Å—Ç—Ä": semester,
            "–î–∏—Å—Ü–∏–ø–ª–∏–Ω–∞": name,
            "–ß–∞—Å—ã": d.get("hours", 144),
            "–§–æ—Ä–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è": d.get("assessment", "—ç–∫–∑–∞–º–µ–Ω"),
            "–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –§–ì–û–°": meta.get("competencies", ""),
            "–¢—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏": meta.get("TF", []),
            "–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ": meta.get("reason", "")
        })

    # 3.5 ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∞–∫—Ç–∏–∫—É –∏ –ì–ò–ê
    rows.append({
        "üõ† –í —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ": "–ë–ª–æ–∫ 2. –ü—Ä–∞–∫—Ç–∏–∫–∞",
        "–°–µ–º–µ—Å—Ç—Ä": 7,
        "–î–∏—Å—Ü–∏–ø–ª–∏–Ω–∞": "–£—á–µ–±–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞",
        "–ß–∞—Å—ã": 108,
        "–§–æ—Ä–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è": "–∑–∞—á—ë—Ç",
        "–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –§–ì–û–°": "",
        "–¢—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏": [],
        "–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ": "–ü—Ä–∞–∫—Ç–∏–∫–∞ –ø–æ –§–ì–û–°"
    })

    rows.append({
        "üõ† –í —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ": "–ë–ª–æ–∫ 2. –ü—Ä–∞–∫—Ç–∏–∫–∞",
        "–°–µ–º–µ—Å—Ç—Ä": 8,
        "–î–∏—Å—Ü–∏–ø–ª–∏–Ω–∞": "–ü—Ä–µ–¥–¥–∏–ø–ª–æ–º–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞",
        "–ß–∞—Å—ã": 108,
        "–§–æ—Ä–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è": "–∑–∞—á—ë—Ç",
        "–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –§–ì–û–°": "",
        "–¢—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏": [],
        "–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ": "–ü—Ä–µ–¥–¥–∏–ø–ª–æ–º–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –ø–æ –§–ì–û–°"
    })

    rows.append({
        "üõ† –í —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ": "–ë–ª–æ–∫ 3. –ì–ò–ê",
        "–°–µ–º–µ—Å—Ç—Ä": 8,
        "–î–∏—Å—Ü–∏–ø–ª–∏–Ω–∞": "–í–ö–†",
        "–ß–∞—Å—ã": 216,
        "–§–æ—Ä–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è": "–∑–∞—â–∏—Ç–∞",
        "–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –§–ì–û–°": "",
        "–¢—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏": [],
        "–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ": "–ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–∞—è –∏—Ç–æ–≥–æ–≤–∞—è –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏—è"
    })

    return rows


def generate_plan_pipeline(df_fgos, tf_struct, match_json, profile_choice, structure_mode="old"):
    """
    –ü–æ–ª–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä:
    1) –≠—Ç–∞–ø 1 ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∏—Å—Ü–∏–ø–ª–∏–Ω (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ)
    2) –≠—Ç–∞–ø 2 ‚Äî —á–∞—Å—ã –∏ —Ñ–æ—Ä–º—ã –∫–æ–Ω—Ç—Ä–æ–ª—è
    3) –≠—Ç–∞–ø 3 ‚Äî –±–ª–æ–∫–∏, —Å–µ–º–µ—Å—Ç—Ä—ã, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    """

    # –≠—Ç–∞–ø 1
    disciplines = generate_disciplines_from_fgos_and_prof(
        df_fgos, tf_struct, match_json, profile_choice
    )
    if not disciplines:
        return pd.DataFrame()

    st.session_state.debug_disciplines = disciplines

    # –≠—Ç–∞–ø 2
    disciplines_with_hours = assign_hours_and_assessment(disciplines)
    if not disciplines_with_hours:
        return pd.DataFrame()

    st.session_state.debug_disciplines_hours = disciplines_with_hours

    # –≠—Ç–∞–ø 3
    plan_rows = assign_semesters_and_blocks(
    disciplines_with_hours,
    df_fgos,
    tf_struct,
    structure_mode="old")

    if not plan_rows:
        return pd.DataFrame()

    df = pd.DataFrame(plan_rows)
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ø–∏—Å–∫–∏ –≤ —Å—Ç—Ä–æ–∫–∏
    df["–¢—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏"] = df["–¢—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏"].apply(lambda x: ", ".join(x) if isinstance(x, list) else x)
    df["–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –§–ì–û–°"] = df["–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –§–ì–û–°"].apply(lambda x: ", ".join(x) if isinstance(x, list) else x)

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–æ–ª–±—Ü–æ–≤
    required_cols = [
        "üõ† –í —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ", "–°–µ–º–µ—Å—Ç—Ä", "–î–∏—Å—Ü–∏–ø–ª–∏–Ω–∞", "–ß–∞—Å—ã",
        "–§–æ—Ä–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è", "–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –§–ì–û–°",
        "–¢—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏", "–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ"
    ]
    for col in required_cols:
        if col not in df.columns:
            df[col] = ""

    df = df[required_cols]

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
    df = df.sort_values(by=["üõ† –í —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ", "–°–µ–º–µ—Å—Ç—Ä", "–î–∏—Å—Ü–∏–ø–ª–∏–Ω–∞"])

    return df



def build_tf_dataframes(tf_struct):
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É {"TF": [...]} –≤ –¥–≤–µ —Ç–∞–±–ª–∏—Ü—ã:
    1) df_tf ‚Äî —Å–ø–∏—Å–æ–∫ —Ç—Ä—É–¥–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π (–∫–æ–¥ + –Ω–∞–∑–≤–∞–Ω–∏–µ)
    2) df_content ‚Äî —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¢–§ (–¥–µ–π—Å—Ç–≤–∏—è, –∑–Ω–∞–Ω–∏—è, —É–º–µ–Ω–∏—è, –¥—Ä.)
    """

    # –ï—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—É—Å—Ç–∞—è –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—ã–µ —Ç–∞–±–ª–∏—Ü—ã
    if tf_struct is None or "TF" not in tf_struct:
        return pd.DataFrame(), pd.DataFrame()

    tf_list = tf_struct.get("TF", [])

    tf_rows = []
    content_rows = []

    for tf in tf_list:
        code = (tf.get("code") or "").strip()
        name = (tf.get("name") or "").strip()

        if not code:
            continue

        # –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –¢–§
        tf_rows.append({
            "–ö–æ–¥ –¢–§": code,
            "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ç—Ä—É–¥–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏": name
        })

        # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
        def add_items(items, label):
            for item in items or []:
                item = (item or "").strip()
                if item:
                    content_rows.append({
                        "–ö–æ–¥ –¢–§": code,
                        "–¢–∏–ø": label,
                        "–ó–Ω–∞—á–µ–Ω–∏–µ": item
                    })

        # –î–æ–±–∞–≤–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
        add_items(tf.get("actions"), "–¢—Ä—É–¥–æ–≤–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ")
        add_items(tf.get("knowledge"), "–ù–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –∑–Ω–∞–Ω–∏–µ")
        add_items(tf.get("skills"), "–ù–µ–æ–±—Ö–æ–¥–∏–º–æ–µ —É–º–µ–Ω–∏–µ")
        add_items(tf.get("other"), "–î—Ä—É–≥–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏")

    df_tf = pd.DataFrame(tf_rows)
    df_content = pd.DataFrame(content_rows)

    return df_tf, df_content



def apply_ai_edit_proposal(user_text, df_current):
    """
    –ò–ò —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –ü–†–ï–î–õ–û–ñ–ï–ù–ò–ï –∏–∑–º–µ–Ω–µ–Ω–∏–π (–ø–æ–ª–Ω—ã–π –Ω–æ–≤—ã–π –ø–ª–∞–Ω).
    –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –∏–ª–∏ –æ—Ç–∫–ª–æ–Ω—è–µ—Ç.
    """

    table_json = df_current.to_dict(orient="records")

    prompt_edit = f"""
–¢—ã ‚Äî –º–µ—Ç–æ–¥–∏—Å—Ç –≤—É–∑–∞, –ø–æ–º–æ–≥–∞—é—â–∏–π –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω.

–¢–µ–∫—É—â–∏–π —É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω (JSON):
{json.dumps(table_json, ensure_ascii=False, indent=2)}

–ó–∞–ø—Ä–æ—Å –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è:
\"\"\"{user_text}\"\"\"


–°—Ñ–æ—Ä–º–∏—Ä—É–π –ü–†–ï–î–õ–û–ñ–ï–ù–ò–ï –∏–∑–º–µ–Ω–µ–Ω–∏–π, –Ω–æ –ù–ï –ø—Ä–∏–º–µ–Ω—è–π –∏—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.
–í–µ—Ä–Ω–∏ —Å—Ç—Ä–æ–≥–æ JSON:

{{
  "comment": "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π",
  "plan_proposed": [
    {{
      "–°–µ–º–µ—Å—Ç—Ä": 1,
      "–î–∏—Å—Ü–∏–ø–ª–∏–Ω–∞": "–ù–∞–∑–≤–∞–Ω–∏–µ",
      "–ß–∞—Å—ã": 144,
      "–§–æ—Ä–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è": "—ç–∫–∑–∞–º–µ–Ω",
      "–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –§–ì–û–°": "–£–ö-1, –û–ü–ö-2",
      "–¢—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏": ["A/01.3"],
      "–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ": "–ö–æ—Ä–æ—Ç–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ"
    }}
  ]
}}

–í–∞–∂–Ω–æ:
- "plan_proposed" –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ü–û–õ–ù–´–ú –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–º –ø–ª–∞–Ω–æ–º.
- –ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏—á–µ–≥–æ –≤–Ω–µ JSON.
"""

    raw = call_yandex_lite(
        [{"role": "user", "text": prompt_edit}],
        temperature=0.25,
        max_tokens=2500
    )

    # –ü–∞—Ä—Å–∏–Ω–≥
    try:
        start = raw.index("{")
        end = raw.rindex("}") + 1
        data = json.loads(raw[start:end])
    except Exception as e:
        return None, f"–û—à–∏–±–∫–∞ —Ä–∞–∑–±–æ—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –ò–ò: {e}. –û—Ç–≤–µ—Ç: {raw[:300]}"

    plan_proposed = data.get("plan_proposed", [])
    comment = data.get("comment", "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è.")

    if not isinstance(plan_proposed, list) or not plan_proposed:
        return None, f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ò–ò: –ø—É—Å—Ç–æ–π –ø–ª–∞–Ω. –û—Ç–≤–µ—Ç: {data}"

    df_new = pd.DataFrame(plan_proposed)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
    required_cols = ["–°–µ–º–µ—Å—Ç—Ä", "–î–∏—Å—Ü–∏–ø–ª–∏–Ω–∞", "–§–æ—Ä–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è"]
    for col in required_cols:
        if col not in df_new.columns:
            return None, f"–í –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–º –ø–ª–∞–Ω–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ '{col}'. –û—Ç–≤–µ—Ç –ò–ò: {data}"

    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ
    for col in ["–ß–∞—Å—ã", "–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –§–ì–û–°", "–¢—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏", "–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ"]:
        if col not in df_new.columns:
            df_new[col] = ""

    df_new = df_new[[
        "–°–µ–º–µ—Å—Ç—Ä", "–î–∏—Å—Ü–∏–ø–ª–∏–Ω–∞", "–ß–∞—Å—ã",
        "–§–æ—Ä–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è", "–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –§–ì–û–°",
        "–¢—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏", "–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ"
    ]]

    return df_new, comment


def main():
    st.set_page_config(
        page_title="–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —É—á–µ–±–Ω–æ–≥–æ –ø–ª–∞–Ω–∞ (Lite, —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è)",
        layout="wide"
    )
    st.markdown("""
<style>

    /* ====== –ì–õ–ê–í–ù–´–ô –§–û–ù ====== */
    .stApp {
        background-color: #f8fbff !important;  /* –±–µ–ª—ã–π —Å –ª—ë–≥–∫–∏–º –≥–æ–ª—É–±—ã–º */
    }

    /* ====== –ó–ê–ì–û–õ–û–í–ö–ò ====== */
    h1, h2, h3 {
        color: #003366 !important;  /* —Ç—ë–º–Ω–æ-—Å–∏–Ω–∏–π */
    }

    /* ====== –û–ë–´–ß–ù–´–ô –¢–ï–ö–°–¢ ====== */
    div, p, span, label {
        color: #003366 !important;
    }

    /* ====== –ö–ù–û–ü–ö–ò ====== */
    .stButton>button {
        background-color: #cc0000 !important;  /* –∫—Ä–∞—Å–Ω–∞—è –∫–Ω–æ–ø–∫–∞ */
        color: white !important;
        border-radius: 8px !important;
        padding: 10px 20px !important;
        font-size: 16px !important;
        border: none !important;
    }

    /* ====== –í–ö–õ–ê–î–ö–ò ====== */
    .stTabs [data-baseweb="tab"] {
        background-color: #e6f0ff !important;  /* —Å–≤–µ—Ç–ª–æ-–≥–æ–ª—É–±–æ–π */
        color: #003366 !important;
        font-weight: bold !important;
        border-radius: 5px !important;
        padding: 10px !important;
        border: 2px solid #003366 !important;
        margin-right: 5px !important;
    }

    /* –§–æ–Ω —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤–∫–ª–∞–¥–æ–∫ */
    div[data-testid="stTabs"] > div {
        background-color: #f8fbff !important;
    }

    /* ====== –ë–û–ö–û–í–ê–Ø –ü–ê–ù–ï–õ–¨ ====== */
    section[data-testid="stSidebar"] {
        background-color: #f0f4ff !important;
        color: #003366 !important;
        border-right: 2px solid #003366 !important;
    }

    /* ====== SELECTBOX (–í–´–ü–ê–î–ê–Æ–©–ò–ô –°–ü–ò–°–û–ö) ====== */

    /* –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä */
    div[data-baseweb="select"] {
        background-color: #ffffff !important;
        border: 2px solid #003366 !important;
        border-radius: 6px !important;
        color: #003366 !important;
    }

    /* –¢–µ–∫—Å—Ç */
    div[data-baseweb="select"] * {
        color: #003366 !important;
    }

    /* –í—ã–ø–∞–¥–∞—é—â–µ–µ –º–µ–Ω—é */
    ul[role="listbox"] {
        background-color: #ffffff !important;
        border: 2px solid #003366 !important;
        border-radius: 6px !important;
    }

    /* –≠–ª–µ–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–∞ */
    ul[role="listbox"] li {
        color: #003366 !important;
    }

    /* –ù–∞–≤–µ–¥–µ–Ω–∏–µ */
    ul[role="listbox"] li:hover {
        background-color: #e6f0ff !important;
        color: #003366 !important;
    }

    /* ====== –ó–û–ù–ê –ó–ê–ì–†–£–ó–ö–ò –§–ê–ô–õ–û–í ====== */

    /* –í–Ω–µ—à–Ω–∏–π –±–µ–ª—ã–π –±–ª–æ–∫ */
    div[data-testid="stFileUploader"] > section {
        background-color: #ffffff !important;
        border: 2px solid #003366 !important;
        border-radius: 8px !important;
        padding: 16px !important;
    }

    /* –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –≥–æ–ª—É–±–æ–π –±–ª–æ–∫ —Å –ø—É–Ω–∫—Ç–∏—Ä–æ–º */
    div[data-testid="stFileUploader"] [data-testid="stFileDropzone"] {
        background-color: #f0f4ff !important;
        border: 2px dashed #003366 !important;
        border-radius: 6px !important;
    }

    /* –¢–µ–∫—Å—Ç –≤–Ω—É—Ç—Ä–∏ –∑–æ–Ω—ã –∑–∞–≥—Ä—É–∑–∫–∏ */
    div[data-testid="stFileUploader"] * {
        color: #003366 !important;
    }

    /* –ö–Ω–æ–ø–∫–∞ Browse file ‚Äî –±–µ–ª–∞—è –Ω–∞ —Ç—ë–º–Ω–æ-—Å–∏–Ω–µ–º */
    div[data-testid="stFileUploader"] button {
        background-color: #ffffff !important;
        color: #003366 !important;
        border: 2px solid #003366 !important;
        border-radius: 6px !important;
        padding: 6px 14px !important;
    }

    /* ====== –¢–ê–ë–õ–ò–¶–´ ====== */
    .stDataFrame {
        background-color: #ffffff !important;
        border: 2px solid #003366 !important;
        border-radius: 6px !important;
        padding: 10px !important;
        color: #003366 !important;
    }

    /* ====== –ß–ê–¢-–°–û–û–ë–©–ï–ù–ò–Ø ====== */
    .stChatMessage {
        background-color: #f9f9f9 !important;
        border-left: 4px solid #cc0000 !important;
        padding: 10px !important;
        margin-bottom: 10px !important;
        color: #003366 !important;
    }
    /* ====== SELECTBOX ‚Äî –ü–û–õ–ù–û–ï –ü–ï–†–ï–ö–†–ê–®–ò–í–ê–ù–ò–ï ====== */

/* –í–Ω–µ—à–Ω–∏–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä selectbox */
div[data-testid="stSelectbox"] {
    background-color: #ffffff !important;
}

/* –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å —Ç–µ–∫—Å—Ç–æ–º */
div[data-testid="stSelectbox"] > div {
    background-color: #ffffff !important;
}

/* –û—Å–Ω–æ–≤–Ω–æ–π –±–ª–æ–∫ select */
div[data-baseweb="select"] {
    background-color: #ffffff !important;
    border: 2px solid #003366 !important;
    border-radius: 6px !important;
    color: #003366 !important;
}

/* –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã */
div[data-baseweb="select"] * {
    background-color: #ffffff !important;
    color: #003366 !important;
}

/* –í—ã–ø–∞–¥–∞—é—â–µ–µ –º–µ–Ω—é */
ul[role="listbox"] {
    background-color: #ffffff !important;
    border: 2px solid #003366 !important;
    border-radius: 6px !important;
}

/* –≠–ª–µ–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–∞ */
ul[role="listbox"] li {
    background-color: #ffffff !important;
    color: #003366 !important;
}

/* –ù–∞–≤–µ–¥–µ–Ω–∏–µ */
ul[role="listbox"] li:hover {
    background-color: #e6f0ff !important;
    color: #003366 !important;
}

/* –ê–∫—Ç–∏–≤–Ω—ã–π –≤—ã–±—Ä–∞–Ω–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç */
ul[role="listbox"] li[aria-selected="true"] {
    background-color: #e6f0ff !important;
    color: #003366 !important;
}

/* –ì–ª—É–±–æ–∫–∏–µ —Å–∫—Ä—ã—Ç—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã (—É–±–∏—Ä–∞—é—Ç —á—ë—Ä–Ω—ã–π —Ñ–æ–Ω –ø–æ–ª–Ω–æ—Å—Ç—å—é) */
div[role="combobox"] {
    background-color: #ffffff !important;
}

div[role="combobox"] * {
    background-color: #ffffff !important;
    color: #003366 !important;
}
/* ====== –ö–ù–û–ü–ö–ò: –±–µ–ª—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ –∫—Ä–∞—Å–Ω–æ–º —Ñ–æ–Ω–µ ====== */
.stButton > button,
.stButton > button * {
    color: #ffffff !important;
}


</style>
""", unsafe_allow_html=True)





    st.title("–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —É—á–µ–±–Ω–æ–≥–æ –ø–ª–∞–Ω–∞")
    st.caption("YandexGPT Lite ‚Ä¢ –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è ‚Ä¢ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–µ–º–µ—Å—Ç—Ä–∞–º ‚Ä¢ –°—Ç–∞—Ä–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (3 –±–ª–æ–∫–∞)")

    tab_normative, tab_generate, tab_editor = st.tabs([
        "–ù–æ—Ä–º–∞—Ç–∏–≤–Ω–∞—è –±–∞–∑–∞ (–§–ì–û–° + –ø—Ä–æ—Ñ—Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã)",
        "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É—á–µ–±–Ω–æ–≥–æ –ø–ª–∞–Ω–∞",
        "–ò–ò‚Äë—Ä–µ–¥–∞–∫—Ç–æ—Ä —É—á–µ–±–Ω–æ–≥–æ –ø–ª–∞–Ω–∞"
    ])

    
    with tab_normative:
        st.header("–ù–æ—Ä–º–∞—Ç–∏–≤–Ω–∞—è –±–∞–∑–∞: –§–ì–û–° + –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã")

        # ---- –§–ì–û–° ----
        st.subheader("1. –ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –§–ì–û–°")

        uploaded_fgos = st.file_uploader(
            "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –§–ì–û–° (PDF)",
            type=["pdf"],
            key="fgos_upload"
        )

        if uploaded_fgos:
            text_fgos = extract_text_from_pdf_file(uploaded_fgos)
            competencies = extract_competencies_full(text_fgos)
            df_fgos = pd.DataFrame(competencies)
            st.session_state.df_fgos = df_fgos

            if df_fgos.empty:
                st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –∏–∑ –§–ì–û–°.")
            else:
                st.success("–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –§–ì–û–° —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω—ã!")
                st.dataframe(df_fgos, use_container_width=True)
                st.download_button(
                    "–°–∫–∞—á–∞—Ç—å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –§–ì–û–° (Excel)",
                    dataframe_to_excel_bytes(df_fgos),
                    "fgos_competencies.xlsx"
                )

        
        st.subheader("2. –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã")

        uploaded_ps = st.file_uploader(
            "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç (PDF)",
            type=["pdf"],
            key="ps_upload"
        )

        if uploaded_ps:
            st.write("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞...")
            full_text = extract_text_from_pdf_file(uploaded_ps)

            st.write("–ò–ò –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä—É–¥–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏...")
            tf_struct, err_ps = analyze_prof_standard(full_text)

            if err_ps:
                st.error(err_ps)
            else:
                st.session_state.ps_struct = tf_struct

                df_tf, df_content = build_tf_dataframes(tf_struct)
                st.session_state.df_tf = df_tf
                st.session_state.df_ps_content = df_content

                st.success("–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω!")

                st.markdown("### –°–ø–∏—Å–æ–∫ —Ç—Ä—É–¥–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π (–¢–§)")
                st.dataframe(df_tf, use_container_width=True)

                st.download_button(
                    "–°–∫–∞—á–∞—Ç—å —Å–ø–∏—Å–æ–∫ –¢–§ (Excel)",
                    dataframe_to_excel_bytes(df_tf),
                    "profstandart_tf_list.xlsx"
                )

                if not df_content.empty:
                    st.download_button(
                        "–°–∫–∞—á–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¢–§ (Excel)",
                        dataframe_to_excel_bytes(df_content),
                        "profstandart_tf_content.xlsx"
                    )

                st.markdown("### –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Ç—Ä—É–¥–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π")
                for _, row in df_tf.iterrows():
                    code = row["–ö–æ–¥ –¢–§"]
                    name = row["–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ç—Ä—É–¥–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏"] or ""
                    header = f"{code} ‚Äî {name}" if name else code

                    with st.expander(header):
                        df_sub = df_content[df_content["–ö–æ–¥ –¢–§"] == code]
                        st.dataframe(df_sub, use_container_width=True)

        
        st.subheader("3. –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –§–ì–û–° ‚Üî –ø—Ä–æ—Ñ—Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã")

        if "df_fgos" in st.session_state and "ps_struct" in st.session_state:
            df_fgos = st.session_state.df_fgos
            tf_struct = st.session_state.ps_struct

            if st.button("–í—ã–ø–æ–ª–Ω–∏—Ç—å —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ"):
                st.write("–ò–ò –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ...")
                match_json, raw_err = match_fgos_and_prof(df_fgos, tf_struct)

                if raw_err:
                    st.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ –ò–ò, –Ω–æ –≤–æ–∑–≤—Ä–∞—â—ë–Ω –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.")
                    st.text_area("–°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç –ò–ò", raw_err, height=150)

                st.session_state.match_json = match_json
                st.success("–°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ!")

                st.write("### –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è")
                st.json(match_json.get("matches", []))

                st.write("### –ù–µ–ø–æ–∫—Ä—ã—Ç—ã–µ –¢–§")
                st.json(match_json.get("gaps", []))

                st.write("### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
                for rec in match_json.get("recommendations", []):
                    st.write("- " + rec)
        else:
            st.info("–î–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑–∏—Ç–µ –§–ì–û–° –∏ –ø—Ä–æ—Ñ—Å—Ç–∞–Ω–¥–∞—Ä—Ç.")

    
    with tab_generate:
        st.header("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É—á–µ–±–Ω–æ–≥–æ –ø–ª–∞–Ω–∞")

        profile_choice = st.selectbox(
            "–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–æ—Ñ–∏–ª—å (–º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å '–ê–≤—Ç–æ')",
            [
                "–ê–≤—Ç–æ",
                "–ü–µ–¥–∞–≥–æ–≥–∏–∫–∞",
                "–Æ—Ä–∏—Å–ø—Ä—É–¥–µ–Ω—Ü–∏—è",
                "–ü—Å–∏—Ö–æ–ª–æ–≥–∏—è",
                "–≠–∫–æ–Ω–æ–º–∏–∫–∞",
                "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
                "–°–æ—Ü–∏–∞–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞",
                "–ú–µ–¥–∏—Ü–∏–Ω–∞",
                "–î—Ä—É–≥–æ–µ"
            ],
            index=0
        )

        if "df_fgos" not in st.session_state or "ps_struct" not in st.session_state:
            st.info("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –§–ì–û–° –∏ –ø—Ä–æ—Ñ—Å—Ç–∞–Ω–¥–∞—Ä—Ç –≤–æ –≤–∫–ª–∞–¥–∫–µ '–ù–æ—Ä–º–∞—Ç–∏–≤–Ω–∞—è –±–∞–∑–∞'.")
        elif "match_json" not in st.session_state:
            st.info("–°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –§–ì–û–° –∏ –ø—Ä–æ—Ñ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞.")
        else:
            df_fgos = st.session_state.df_fgos
            tf_struct = st.session_state.ps_struct
            match_json = st.session_state.match_json

            if st.button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω"):
                st.write("–ò–ò —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω...")

                df_plan = generate_plan_pipeline(
                    df_fgos,
                    tf_struct,
                    match_json,
                    profile_choice,
                    structure_mode="old"
                )
                st.session_state.df_plan = df_plan

                st.success("–£—á–µ–±–Ω—ã–π –ø–ª–∞–Ω —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω!")
                st.dataframe(df_plan, use_container_width=True)

                st.download_button(
                    "–°–∫–∞—á–∞—Ç—å —É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω (Excel)",
                    dataframe_to_excel_bytes(df_plan),
                    "uchebny_plan.xlsx"
                )

    with tab_editor:
        st.header("–ò–ò‚Äë—Ä–µ–¥–∞–∫—Ç–æ—Ä —É—á–µ–±–Ω–æ–≥–æ –ø–ª–∞–Ω–∞")

        if "df_plan" not in st.session_state or st.session_state.df_plan.empty:
            st.info("–°–Ω–∞—á–∞–ª–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ —É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω.")
            return

        df_current = st.session_state.df_plan
        st.subheader("–¢–µ–∫—É—â–∏–π —É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω")
        st.dataframe(df_current, use_container_width=True)

        st.markdown("---")
        st.subheader("–ß–∞—Ç —Å –ò–ò‚Äë–º–µ—Ç–æ–¥–∏—Å—Ç–æ–º")

        if "chat_messages" not in st.session_state:
            st.session_state.chat_messages = []

        if "pending_edit_df" not in st.session_state:
            st.session_state.pending_edit_df = None
        if "pending_comment" not in st.session_state:
            st.session_state.pending_comment = None
        if "awaiting_confirmation" not in st.session_state:
            st.session_state.awaiting_confirmation = False

        # –≤—ã–≤–æ–¥ –∏—Å—Ç–æ—Ä–∏–∏
        for msg in st.session_state.chat_messages:
            with st.chat_message(msg["role"]):
                st.write(msg["content"])

        user_input = st.chat_input("–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –∫ –ò–ò‚Äë–º–µ—Ç–æ–¥–∏—Å—Ç—É...")

        if user_input:
            # –µ—Å–ª–∏ –∂–¥—ë–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
            if st.session_state.awaiting_confirmation:
                st.session_state.chat_messages.append({"role": "user", "content": user_input})
                with st.chat_message("user"):
                    st.write(user_input)

                low = user_input.strip().lower()
                if low in ["–¥–∞", "yes", "–æ–∫", "–ø—Ä–∏–º–µ–Ω–∏—Ç—å"]:
                    st.session_state.df_plan = st.session_state.pending_edit_df
                    st.session_state.pending_edit_df = None
                    st.session_state.pending_comment = None
                    st.session_state.awaiting_confirmation = False
                    answer = "–ò–∑–º–µ–Ω–µ–Ω–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω—ã."
                elif low in ["–Ω–µ—Ç", "no", "–æ—Ç–º–µ–Ω–∞"]:
                    st.session_state.pending_edit_df = None
                    st.session_state.pending_comment = None
                    st.session_state.awaiting_confirmation = False
                    answer = "–ò–∑–º–µ–Ω–µ–Ω–∏—è –æ—Ç–º–µ–Ω–µ–Ω—ã."
                else:
                    answer = "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–≤–µ—Ç—å—Ç–µ '–¥–∞' –∏–ª–∏ '–Ω–µ—Ç'."

                st.session_state.chat_messages.append({"role": "assistant", "content": answer})
                with st.chat_message("assistant"):
                    st.write(answer)

                st.rerun()

            # –µ—Å–ª–∏ —ç—Ç–æ –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            else:
                st.session_state.chat_messages.append({"role": "user", "content": user_input})
                with st.chat_message("user"):
                    st.write(user_input)

                st.write("–ò–ò —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π...")

                df_proposed, comment = apply_ai_edit_proposal(user_input, df_current)

                if df_proposed is None:
                    st.session_state.chat_messages.append({"role": "assistant", "content": comment})
                    with st.chat_message("assistant"):
                        st.write(comment)
                else:
                    st.session_state.pending_edit_df = df_proposed
                    st.session_state.pending_comment = comment
                    st.session_state.awaiting_confirmation = True

                    answer = (
                        f"{comment}\n\n"
                        "–ü–æ–∫–∞–∑–∞–Ω –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã–π –≤–∞—Ä–∏–∞–Ω—Ç —É—á–µ–±–Ω–æ–≥–æ –ø–ª–∞–Ω–∞ –Ω–∏–∂–µ. "
                        "–ü—Ä–∏–º–µ–Ω–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è? (–¥–∞ / –Ω–µ—Ç)"
                    )
                    st.session_state.chat_messages.append({"role": "assistant", "content": answer})
                    with st.chat_message("assistant"):
                        st.write(answer)

                    st.subheader("–ü—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã–π –≤–∞—Ä–∏–∞–Ω—Ç (–µ—â—ë –Ω–µ –ø—Ä–∏–º–µ–Ω—ë–Ω)")
                    st.dataframe(df_proposed, use_container_width=True)

                st.rerun()


if __name__ == "__main__":
    main()
